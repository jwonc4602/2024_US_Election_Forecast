---
title: "A Polls-of-Polls Forecast for the 2024 United States Presidential Election Through Generalized Linear Model"
subtitle: "The Impact of Pollster Differences, Population Type, and Poll Regency on Predicting Support for Kamala Harris"
author: 
  - Jiwon Choi
  - Kevin Roe
thanks: "Code and data are available at: [https://github.com/jwonc4602/2024_US_Election_Forecast](https://github.com/jwonc4602/2024_US_Election_Forecast)."
date: today
date-format: long
abstract: "First sentence. Second sentence. Third sentence. Fourth sentence."
format: pdf
number-sections: true
toc: true
bibliography: references.bib
---

```{r}
#| include: false
#| warning: false
#| message: false

library(tidyverse)
library(here)
library(kableExtra)
library(broom.mixed)
library(knitr)
library(kableExtra)
library(rstanarm)

#### Read in necessary data and model ####
cleaned_president_polls <-read_csv(file = here("data/02-analysis_data/cleaned_president_polls.csv"), show_col_types = FALSE)
cleaned_data_survey <- read_csv(file = here("data/02-analysis_data/national_polling.csv"), show_col_types = FALSE)
state_polling_data <- read_csv(file = here("data/02-analysis_data/state_polling_data.csv"), show_col_types = FALSE)
state_analysis <- read_csv(file = here("data/02-analysis_data/state_analysis.csv"), show_col_types = FALSE)

harris_model <- readRDS(file = here("models/vote_for_Harris_model.rds"))
prediction <- read_csv(file = here("data/02-analysis_data/vote_for_Harris_predictions.csv"), show_col_types = FALSE)
```

# Introduction

The United States Presidential Election is one of the most consequential events of 2024. As a key country in international relations and the largest economy in the world, the results of the U.S election determines the country's domestic and foreign policy for the next four years, which will have a significant impact on important initiatives such as tackling climate change or international conflicts. In anticipation of the 2024 U.S election, this paper aims to predict the possible outcomes of the election by analyzing the level of support that Kamala Harris will gain.

We forecast support for Kamala Harris based on polling results and apply a Bayesian generalized linear model. The main parameter of interest is the proportion of vote or support that Harris received in surveys, which is traced over time. By considering the results from different poll-making organizations and other demographic factors, our objective is to correct for variation across different voter bases with

Factoring in the different results of poll-making organizations, our model found that specific pollsters or demographic characteristics increase or decrease Harris' predicted voter share. Understanding the trajectory of the US might make in foreign policy, economic or global politics prediction allows global stakeholders to take preemptive measures for policy changes of the newly elected government. Thus, this study is a strong tool to navigate uncertainty around the 2024 election.

The paper is structured as follows: @sec-data and @sec-model explores the data and methodology used, highlighting the filtering and modeling techniques applied to the data; @sec-results presents the results from the Bayesian generalized linear model; @sec-discussion discusses the broader implications of our findings; and @sec-appendix highlights YouGov's methodology, outlines an idealized survey methodology, and shows model diagnostics.

# Data {#sec-data}

We have used a poll-of-polls of the upcoming US presidential election dataset obtained from FiveThirtyEight [@fivethirtyeight_polls]. Data was collected and analyzed using R statistical programming software [@citeR], with additional packages like tidyverse [@tidyverse], rstanarm [@rstanarm] knitr [@citeKnitr], here [@here], and many others for support. The dataset includes a wide range of poll results from various polls, with keyv ariables such as pollster, sample size, percentage of support for Harris, and the date conducted for polls.

To ensure high data quality, we filtered the dataset to include polls with a numeric grade of 2.7 or above and focused only on polls conducted after July 21, 2024, the date of Harris' declaration of candidacy. The filter helped limit bias from older polls from when Joe Biden was the Democratic nominee, which may not reflect the current voter sentiment.

In performing the analysis, we used several R pacakages. We used tidyverse [@tidyverse] for data manipulation and visualization, and rstanarm [@rstanarm] for Bayesian modeling. To visualize results, we used ggplot2 [@ggplot2] and kableExtra [@citekableExtra] to format tables for presentation.

## Measurement {#sec-data-measurement}

FiveThirtyEight aggregates various poll results from national and state-wide polls, showing 16867 observations [@fivethirtyeight_polls]. The poll takes a sample of the electorate and asks for the voters' candidate of choice. By factoring polls from a state and national level, FiveThirtyEight aims to represnet public perception on the two candidates during the election to predict the election's outcome.

Each poll aims to predict an actual event. The raw data is susceptible because each pollster has varying polling methods. There are other limitations such as sampling error, response error, or distortion from participants misunderstanding the question.

Beyond filtering for high-quality pollsters, we filtered for various criteria to prepare the analysis data.From the raw data, we filtered out any missing numbers, cleaned names, and removed all polls on non-Democrat or Republican candidates, effectively on comparing Harris and Trump. After determining state-level analysis for electoral votes and close races, we created different datasets for national and state level polls. Regardless, selection bias and sampling bias is still a concern because polls only represent a part of the population. Finally, polling methodology difference can introduce bias in the study. Overall, we use the data from individual responses to election polls, and filter the data for analysis to make a prediction on who will win the popular vote based on data from the pollsters.

## Outcome variables {#sec-data-outcome}

### The Predicted Proportion of Support a Candidate Received in a Poll

The main variable we aim to forecast is the pct variable, which represents the proportion of the vote a candidate received in a poll.@tbl-pct-cleaned and @fig-pct-cleaned shows the summary statistics and distribution of pct variable in a filtered dataset that only comprises of votes from relatively high-quality polling organizations. We also get the popular vote predictions for each candidate using the predict() function and add this to the dataset as the predicted_pct variable. We have showed the summary statistics and distribution for predicted_pct in @tbl-pct-predicted and @fig-pct-predicted, respectively.

Comparing our predictions to the data, our predictions have a smaller range than the cleaned data. While the difference in mean is 2% lower for our prediction than our cleaned data. The results suggest there is less variation in our predictions than the cleaned dataset. 

```{r}
#| label: tbl-pct-cleaned
#| tbl-cap: Summary statistics for the proportion of support candidates received in the poll
#| echo: false
#| message: false
#| warning: false
#| tbl-align: center
#| tbl-width: 82%


summary_stats <- cleaned_president_polls %>%
  summarize(
    mean = round(mean(pct, na.rm = TRUE), 2),
    median = round(median(pct, na.rm = TRUE), 2),
    min = round(min(pct, na.rm = TRUE), 2),
    max = round(max(pct, na.rm = TRUE), 2), 
    sd = round(sd(pct, na.rm = TRUE), 2), 
    n = n()
  )

summary_stats %>%
  kable() %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"), full_width = F)

```

```{r}
#| label: fig-pct-cleaned
#| fig-cap: Distribution of the proportion of support candidates received in the poll
#| echo: false
#| message: false
#| warning: false


# Plot distribution using histogram
ggplot(cleaned_president_polls, aes(x = pct)) + 
  geom_histogram(binwidth = 3, fill = "green", alpha = 0.7, color = "black") +
  theme_minimal() + 
  labs(x = "Percentage of Support (pct)", 
       y = "Count")
```

```{r}
#| label: tbl-pct-predicted
#| tbl-cap: Summary statistics for the predicted proportion of support candidates received in the poll
#| echo: false
#| message: false
#| warning: false
#| tbl-align: center
#| tbl-width: 82%


# Calculate summary statistics for the 'pct' variable
summary_stats <- prediction %>%
  summarize(
    mean = round(mean(predicted_pct, na.rm = TRUE), 2),
    median = round(median(predicted_pct, na.rm = TRUE), 2),
    min = round(min(predicted_pct, na.rm = TRUE), 2),
    max = round(max(predicted_pct, na.rm = TRUE), 2), 
    sd = round(sd(predicted_pct, na.rm = TRUE), 2), 
    n = n()

  )

# Create a formatted table to display summary statistics 
summary_stats %>%
  kable() %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"), full_width = F)

```

```{r}
#| label: fig-pct-predicted
#| fig-cap: Distribution of the predicted proportion of support candidates received in the poll
#| echo: false
#| message: false
#| warning: false
#| fig-width: 5 
#| fig-align: center



# Plot distribution using histogram
ggplot(prediction, aes(x = predicted_pct)) + 
  geom_histogram(binwidth = 3, fill = "green", alpha = 0.7, color = "black") +
  theme_minimal() + 
  labs(x = "Predicted Percentage of Support (predicted_pct)", 
       y = "Count")
```

## Predictor variables {#sec-data-predictor}

### Type of Pollster {sec-data-pollster}

The pollster variable was selected to consider the effect of changes in pollster on the observed variable. The pollster variable represents the polling organization that conducted the poll. The distribution of polling counts for different pollsters in @fig-pollster suggests that the data is dominated by three pollsters: Ipsos, YouGov and Siena/NYT. Further analysis is needed in their polling methodology to determine potential biases. 

```{r}
#| label: tbl-pollster
#| fig-cap: Number of unique high-quality polling organizations
#| echo: false
#| tbl-align: center
#| tbl-width: 82%


# Calculate summary statistics for the 'pollster' variable
summary_stats <- prediction %>%
  summarize(
    count(pollster, name = "Count")
  )

# Display the summary statistics in a nicely formatted table
summary_stats %>%
  kable() %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"), full_width = F)

```

```{r}
#| label: fig-pollster
#| fig-cap: Distribution of high-quality polling organizations
#| echo: false
#| fig-width: 5 
#| fig-align: center



# Plot distribution using histogram
ggplot(prediction, aes(x = pollster)) +
  geom_bar(fill = "blue", alpha = 0.7, color = "black") +
  theme_minimal() +
  labs(x = "Pollster",
       y = "Count") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
```


### Population Type {sec-data-populationtype}

Population Type distinguishes among different groups surveyed into two groups surveyed: voters and adults. Adults is a more general poll that includes voters, likely voters and non-voters. @fig-populationtype shows that majority of the polls surveyed voters rather than the general adult population. The discrepancy between number of polls who surveyed voters and adults could introduce potential biases in responses, as those who are voters may have stronger opinions than those who are not. 

```{r}
#| label: tbl-populationtype
#| fig-cap: Number of unique high-quality polling organizations
#| echo: false
#| tbl-align: center
#| tbl-width: 82%


# Calculate summary statistics for the 'population' variable
summary_stats <- prediction %>%
  count(population, name = "Count")

# Display the summary statistics in a nicely formatted table
summary_stats %>%
  kable() %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"), full_width = F)

```

```{r}
#| label: fig-populationtype
#| fig-cap: Distribution of population type 
#| echo: false
#| fig-width: 5 
#| fig-align: center



# Plot distribution of Population Type using histogram
ggplot(prediction, aes(x = population)) +
  geom_bar(fill = "orange", alpha = 0.7, color = "black") +
  theme_minimal() +
  labs(x = "Population",
       y = "Count") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
```

### Poll Recency {sec-data-pollrecency}

Poll Recency, shown through the variable `recent_poll`, labels any polls that have been conducted in the last 30 days as Recent and any polls collected before that are labeled Older. Based on our results shown in @tbl-recency, there are 32 less polls collected in the last 30 days than before. This introduces biases in responses as older responses are more represented in our model. In rapidly changing political environments, public opinion can shift dramatically due to major events, debates or crises, making recent data crucial for accurate modeling.The distribution of this variable can also be found in @fig-recency.


```{r}
#| label: tbl-recency
#| fig-cap: Number of polls that were collected within 30 days and after
#| echo: false
#| tbl-align: center
#| tbl-width: 82%


# Calculate summary statistics for the 'recent_poll' variable
summary_stats <- prediction %>%
  count(recent_poll, name = "Count")


# Display the summary statistics in a nicely formatted table
summary_stats %>%
  kable() %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"), full_width = F)

```

```{r}
#| label: fig-recency
#| fig-cap: Distribution of on the amount of polls collected in 30 days 
#| echo: false
#| fig-width: 5 
#| fig-align: center

# Plot distribution of Population Type using histogram
ggplot(prediction, aes(x = recent_poll)) +
  geom_bar(fill = "purple", alpha = 0.7, color = "black") +
  theme_minimal() +
  labs(x = "Poll Recency",
       y = "Count") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
```



# Model {#sec-model}

For our analysis, we employ a Bayesian generalized linear model (GLM) to forecast the popular vote percentage for Kamala Harris. This approach allows us to capture polling characteristics and account for known variations between pollsters, population types, and the recency of polling data. By incorporating these factors, we aim to model a nuanced estimate of Harris's projected popular vote.

The first step in our process involved selecting a reliable dataset for model development. Here, we utilized high-quality national polling data gathered after Harris’s campaign announcement. We filtered the dataset to include polls with a numeric grade of 2.7 or above, ensuring data quality, and focused only on polls conducted after July 21, 2024, the date of Harris’s declaration of candidacy. This filter helps limit bias from older polls that may not reflect the current voter sentiment.

The GLM is specified as follows: \begin{equation}
Y_i = \beta_{0} + \beta_{1}x_{pollster_i} + \beta_{2}x_{population_i} + \beta_{3}x_{recentpoll_i} + \epsilon_{i}
\label{eq:logit}
\end{equation}

In equation \ref{eq:logit}, each $\beta$ represents a coefficient determined through regression analysis. The variables chosen for this project are pollster, population type, and recency of the poll. Each predictor variable was carefully selected based on its significance in polling analysis and its correlation with voting trends. The identity of the polling organization is important to our model, as each pollster may exhibit unique biases. Including Pollster as a fixed effect allows us to account for these variations without introducing unnecessary complexity. Population Type distinguishes among different groups surveyed (e.g., voters, likely voters) and helps in capturing generalizability. Categorizing polls as either recent or not ensures that more recent polls, which are better predictors of voting behavior closer to the election, receive appropriate emphasis. $Y_i$ denotes the predicted popular vote percentage for Kamala Harris in the $i$-th poll. $\epsilon_i$ is the Gaussian-distributed error term, accounting for residual variation in the model.

The selection of predictors in this model is based on observable patterns in the data, such as the consistent tendencies of certain pollsters and the impact of population type on polling outcomes. To enhance the model's robustness, Bayesian priors were applied, introducing regularization and incorporating plausible ranges grounded in previous election data and polling analysis. For the coefficient priors $\beta$, a normal distribution with a mean of 0 and a scale of 2.5 (autoscaled) was chosen to provide flexibility while mitigating overfitting. Similarly, the intercept uses a normal prior with a mean of 0 and scale of 2.5 to stabilize model estimates. For the error term (sigma), an exponential prior with a rate of 1 was selected to constrain the residuals, aligning with Gaussian assumptions. These priors offer a balance between model flexibility and constraint, informed by established trends in polling data and comparable electoral forecasts.

The model was implemented in R [@citeR] using the `rstanarm` package, which offers an accessible interface for Bayesian generalized linear models (GLMs), allowing specification of priors and customization of model parameters. Once the logistic regression model for predicting the popular vote is developed, we apply the `predict()` function in R [@citeR] to generate popular vote percentage predictions for each candidate, using filtered national data. These predictions are then added to the dataset as a new variable (`predicted_pct`) for further analysis. The results are saved in a CSV file (`popular_vote_predictions.csv`), enabling an in-depth examination of potential outcomes in the popular vote. This approach facilitates understanding of vote distributions at a national level and provides a foundation for forecasting electoral outcomes based on demographic and polling data.

# Results {#sec-results}

To assess model reliability, we examined several key diagnostics. Convergence metrics, such as Rhat values, were close to 1 for all parameters, indicating strong convergence. Additionally, the effective sample size (n_eff) was high across parameters, suggesting low autocorrelation and contributing to model stability. For validation, we conducted out-of-sample testing and calculated the Root Mean Square Error (RMSE) to assess predictive accuracy. This test data, which includes polls not used in training, provides an unbiased estimate of model performance. See more details of model diagnostic here: @sec-model-details-diagnostics.

The model operates under the assumption that residuals follow a Gaussian distribution, though this may not entirely capture extreme polling variances. Additionally, pollster effects are treated as fixed, which, while limiting the model’s ability to reflect dynamic shifts in polling methodologies, simplifies the model and reduces the risk of overfitting. Alternative specifications, including a hierarchical model with random intercepts for pollsters, were considered. However, the added complexity of this approach did not much improve predictive accuracy. The chosen GLM specification thus offered a balanced approach, optimizing both interpretability and performance, making it the preferred model for our study.

```{r}
#| echo: false
#| message: false
#| warning: false
#| label: tbl-model-coefficients
#| tbl-cap: Coefficients from the GLM Model

# Tidy and round the model summary data
coefficients <- broom.mixed::tidy(harris_model, conf.int = TRUE) %>%
  mutate(across(c(estimate, std.error, conf.low, conf.high), round, 2))

# Generate the LaTeX table with 2 decimal places
kable(coefficients, format = "latex", booktabs = TRUE, align = "c") %>% 
  kable_styling(latex_options = "scale_down", font_size = 7)
```

@tbl-model-coefficients presents the estimated coefficients for the predictors in our GLM model. These coefficients fit into the GLM equation, allowing us to interpret the impact of each predictor on Harris’s predicted vote percentage. Positive values indicate a higher predicted percentage, while negative values indicate a decrease. Key predictors, such as specific pollsters and population types, show distinct effects on the forecasted outcomes.

```{r}
#| echo: false
#| message: false
#| warning: false
#| label: fig-coefficient-estimates
#| fig-cap: Coefficient Estimates for Predictors
coefficients %>%
  ggplot(aes(estimate, term)) + 
  geom_point() + 
  geom_errorbar(aes(xmin = conf.low, xmax = conf.high)) + 
  labs(x = "Estimate", y = "Predictor") +
  theme(axis.text.y = element_text(size = 7))
```

@fig-coefficient-estimates represents the model coefficients, with error bars indicating the confidence interval for each estimate. Positive coefficients suggest that specific pollsters or demographic characteristics increase Harris’s predicted vote share. For example, the 'populationVoters' shows a positive impact, while most of the pollsters has a slight negative effect. These error bars help contextualize the reliability of each predictor.

# Discussion {#sec-discussion}

## How has the Forecast Changed Over Time? {#sec-first-point}
```{r}
filtered_prediction <- prediction %>%
  filter(national_poll == 1)

# Plot the graph with both predicted and real percentages
ggplot(filtered_prediction, aes(x = end_date)) +
  geom_line(aes(y = predicted_pct, color = "Predicted Percentage")) +
  geom_line(aes(y = pct, color = "Real Percentage")) +
  labs(
       x = "Date",
       y = "Percentage",
       color = "Legend") +
  theme_minimal()
```

asdfasdfasdfasdf

## Which States Prefer Harris? {#sec-second-point}
```{r}
state_data <- prediction %>% filter(national_poll == 0)
# Identify missing data
missing_data_summary <- prediction %>%
  group_by(state) %>%
  summarize(missing_dates = sum(is.na(predicted_pct)),
            total_dates = n())

print(missing_data_summary)

# Plot with points for sparse data
ggplot(prediction, aes(x = end_date, y = predicted_pct, color = state)) +
  geom_line(na.rm = TRUE) +
  geom_point(na.rm = TRUE) +  # Add points to show actual data entries
  facet_wrap(~ state, scales = "free_y") +
  labs(
       x = "Date",
       y = "Predicted Percentage (%)") +
  theme_minimal() +
  theme(legend.position = "none",
        strip.text = element_text(size = 10))  # Increase facet label size

```

```{r}
# Filter to keep only the most recent entry for each state based on 'end_date'
filtered_data <- prediction %>%
  filter(!is.na(state)) %>%
  arrange(state, end_date) %>%
  group_by(state) %>%
  slice_tail(n = 1) %>%
  ungroup() %>%
  select(state, end_date, predicted_pct)

# Plot the bar chart of predicted percentage for each state with pastel colors
ggplot(filtered_data, aes(x = reorder(state, predicted_pct), y = predicted_pct, fill = state)) +
  geom_bar(stat = "identity") +
  geom_text(aes(label = round(predicted_pct, 1)), hjust = -0.1) + # Display values above bars
  labs(
    x = "Predicted %",
    y = "State"
  ) +
  theme_minimal() +
  coord_flip() + # Flip coordinates for better readability
  scale_fill_manual(values = scales::hue_pal()(nrow(filtered_data))) + # Pastel-like palette
  theme(legend.position = "none") # Remove legend for simplicity
```

## Implications {#sec-implications}

## Weaknesses and Next Steps {#sec-weaknesses}

-   if filter thorugh state, data not collected easily

Weaknesses and next steps should also be included.

\newpage

\appendix

# Appendix {#sec-appendix}

## YouGov Methodology Analysis {#sec-appendix-pollster}

YouGov employs a panel-based methodology for conducting online surveys, recruiting participants who voluntarily join their platform. Their surveys span topics like politics, policy opinions, and voter behavior. In the context of election polling, YouGov applies Multilevel Regression and Post-Stratification (MRP) models to predict outcomes by blending respondent data with external data like voter files [@yougov_mrp_2024].

The target population for YouGov’s surveys typically consists of U.S. adults, with specific samples drawn for each survey depending on the topic. YouGov’s sample frame consists of individuals who have opted into their online panel. To ensure representativeness, YouGov sets demographic quotas, stratifying by variables such as age, gender, and political affiliation. Additionally, YouGov monitors and adjusts their sample post-survey using MRP to correct for demographic imbalances and potential sampling bias.

YouGov’s panel is recruited through various online methods, including digital advertisements and direct outreach. This recruitment strategy leads to a convenience sample, meaning participants self-select to join the panel, which can introduce selection bias. Although YouGov attempts to offset this bias using quotas and post-stratification weighting, the sample still primarily consists of individuals comfortable with online surveys. This method may under-represent certain demographics, such as older adults or individuals with limited internet access.

YouGov’s use of non-probability sampling—drawing respondents from a convenience sample—provides cost efficiency and quick data collection. However, the trade-off lies in the introduction of potential biases due to self-selection. While YouGov’s MRP model corrects some biases by leveraging population-level data for small samples, the non-random nature of the sample may still skew results. For instance, respondents who engage in online surveys may systematically differ from those who avoid them, such as individuals with lower levels of social trust.

Non-response bias is one of the main concerns in YouGov’s methodology. Individuals who opt out of surveys may systematically differ from those who participate. YouGov employs several strategies to mitigate this issue, including post-survey weighting, which adjusts the survey data to reflect population characteristics, and imputation techniques, which estimate missing responses based on patterns in the available data. Despite these efforts, non-response bias can still affect the accuracy of predictions, particularly when certain demographic groups (e.g., less-educated or low-trust individuals) are less likely to participate.

YouGov focuses on clear and concise survey design, reducing participant fatigue and improving data quality. Surveys typically avoid overly complex or leading questions, which can skew the results. However, question-wording remains an important area, as even subtle changes in expression can affect responses. For example, YouGov's research shows how question-wording can affect voter perceptions and reported support for policies. Furthermore, prolonged surveys can lead to increased dropout rates, potentially resulting in non-response bias. To counteract this, YouGov often dynamically adjusts questions based on previous responses to keep participants engaged by incorporating survey logic.

YouGov’s panel-based methodology, combined with statistical techniques like MRP, offers a robust approach to polling. Their methods effectively address many challenges associated with non-random sampling and non-response bias. However, limitations persist, particularly regarding the biases inherent in self-selection and question framing. Overall, while YouGov’s methodology is well-suited to large-scale surveys, particularly in politics, careful consideration of their sampling and non-response strategies is necessary to fully interpret their findings.

## Idealized Methodology {#sec-appendix-methodology}

Because the US operates on the Electoral College system, where state-affiliation matters, I would use Stratified Random Sampling to ensure each state is represented proportionally in the survey, improving accuracy and representativeness. My target population would be voter-eligible citizens in the United States who are above 18 years old. My sampling frame will use voter registration databases that are stratified by key demographics (age, race, gender, geographic region, income, and education level). Finally for my potential sample size, I will aim for around 10,000 respondents to ensure a reasonable margin of error, but I expect the actual sample to be less.

I would primarily recruit respondents using online methods such as surveys on social media platforms like Facebook and Instagram to reach younger audiences and use interactive voice response systems on the phone to reach older or less tech-savvy demographics.

To ensure the data aggregated are valid, I would make sure to cross-check responses with voter registration databases and implement various logic checks within the survey to detect inconsistent answers. For example, if a survey respondent claims to have already voted but stated they’re unlikely to vote makes the data response unreliable.

The biggest concern with online surveys is to handle non-response bias. To mitigate this, I will make sure to over-sample underrepresented groups and use multiple attempts to contact individuals online or run weekly polls. By running frequent surveys, I will make sure to aggregate results using a moving average in the summary results dashboard to smooth out short-term fluctuations. I will also make sure to develop checks online to prevent someone from filling out the form more than once or track the different people we called to ensure we do not have duplicate entries. Finally, because the United States does not run elections using a popular vote, I will weight the survey results to reflect the US voting population based on US Census data and voter turnout estimates, while also factoring demographic factors such as race, gender, and age.

To allocate the $100,000 budget, I would budget \$20,000 for a survey platform subscription to handle a large amount of data. I would put \$50,000 into advertising and recruitment for targeted digital ads and phone surveys. I will also use \$20,000 for data analysts and poll aggregation services and \$10,000 for re-contact surveys, contingency costs, and other miscellaneous costs. However, I believe \$100,000 is on the lower end to ensure a high-quality poll and will most likely cost more. Regardless, most of the costs will come from collecting the data, and I have budgeted for this reality.

Please find the proposed question list for the online survey attached here (INSERT URL).

## Diagnostics for model {#sec-model-details-diagnostics}

@fig-ppcheck compares observed data (dark line) with replicated posterior predictions (lighter lines). The close alignment suggests that the model accurately captures the data's central tendency and variability. @fig-convergencecheckTrace and @fig-convergencecheckRhat show that the sampling algorithm used, the Markov chain Monte Carlo (MCMC) algorithm, did not run into issues as the posterior distribution for the model was created. Using the checks presented by @citetellingstorieswithdata, both graphs do not show anything abnormal since he trace plots in @fig-convergencecheckTrace display substantial horizontal fluctuation across chains, indicating good mixing, while the Rhat values in @fig-convergencecheckRhat are close to 1 and well below 1.1, further supporting convergence.

```{r}
#| echo: false
#| eval: true
#| message: false
#| warning: false
#| label: fig-ppcheck
#| fig-cap: "Posterior Predictive Check: Comparison of Observed and Replicated Data"
pp_check(harris_model) + theme_minimal()
```

```{r}
#| echo: false
#| eval: true
#| message: false
#| warning: false
#| label: fig-convergencecheckTrace
#| fig-cap: "Checking the convergence of the MCMC algorithm - Trace"
plot(harris_model, "trace") + theme_minimal()
```

```{r}
#| echo: false
#| eval: true
#| message: false
#| warning: false
#| label: fig-convergencecheckRhat
#| fig-cap: "Checking the convergence of the MCMC algorithm - Rhat"
plot(harris_model, "rhat") + theme_minimal()
```

\newpage

# References
